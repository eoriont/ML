{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from re import I\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 10\n",
        "max_iters = 5000\n",
        "n_head = 6\n",
        "n_layer = 2\n",
        "dropout = 0.2\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5pqojMJmFwD",
        "outputId": "dba1f78e-d33c-4b7d-8b7f-7b6364b6b11f"
      },
      "outputs": [],
      "source": [
        "# shakespear dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sJhuhBCPmSFm"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([301829]) torch.int64\n",
            "tensor([ 3488,  7161,   349,  5685,   408,  5738,   678,  3131,     5,  4145,\n",
            "          552,  4028,   233,  1797,   349, 11796,     5,  4028,   233,  3488,\n",
            "         7161,   349,  1922,   360,   493,  8942,  3201,   164,  2015,   844,\n",
            "          164,  1604,   604,  1501,  1797,   349,   723,     7,  8942,   233,\n",
            "         3488,  7161,   349,  3488,     5,   339,  1095,   207,  1639,   206,\n",
            "         2099,  3747,   206,   225,  5666,  5242,   164,   136,   970,   233,\n",
            "         1797,   349,  1285,  1095,   722,     5,   408,  1095,   722,   233,\n",
            "         3488,  7161,   349,  5618,   423,  3568,  1110,     5,   175,   408,\n",
            "         2351,   434,  7008,   354,   798,  1407,  2398,   443,  2695,   722,\n",
            "          121,  2590,  1501,  1797,   349,  2019,   596,  4459,   240,   722,\n",
            "           12,   832,   278,   238,  2063,    11,  2255,     5,  2255,  1687,\n",
            "         7671,  7161,   349,  2763,  2437,     5,  1292,  5710,   233,  3488,\n",
            "         7161,   349,  1285,   360,  5124,  4667,  5710,     5,   136,  2348,\n",
            "         1684,  3504,  1292,   443,  2678,  6095,  1335,  1433,   925,   240,\n",
            "          794,  7680,   423,    11,   269,   600,    94,  4937,  4522,   423,\n",
            "          521,   136,  1711, 10739,   331,     5,  1078,   278,   792,    94,\n",
            "         1011,  4366,   453,     5,   408,  1899,  4702,   600,  8505,   423,\n",
            "         2625,   744,   137,  4772,   600,  1350,   408,   360,  1697, 10233,\n",
            "           11,   136,   351,  8152,   276,    94,  1953, 10109,   423,     5,\n",
            "          136,  1270,   168,   798, 10397,     5,   225,   284,   304,    94,\n",
            "        11973,   164,  2754,   823,   651,  3130,    12,   798,    94,    64,\n",
            "         1572,   496,   225,   121,  5035,   164,   856,  4168,   423,  2958,\n",
            "          267,   294,    94,   263,   138,  6556,     5,  4021,   408,  2558,\n",
            "          281,  1577,    11,   220,   136, 11423,  1095,   209,    94,    64,\n",
            "         9836,   267,   157,  1605,   220,  7776,     5,   370,   157,  8244,\n",
            "          220,  2958,   233,  7671,  7161,   349, 11224,   339,  5738,  3475,\n",
            "         1761,   207,  1639,   206,  2099,  3747,   206,  1501,  1797,   349,\n",
            "         8831,  1110,   889,    11,   395,   419,   121,  1239,  3598,   164,\n",
            "          136,  2880,  5610,   233,  7671,  7161,   349,  3508,   339,   871,\n",
            "         2500,   395,   510,  2063,   220,   599,  2271,  1501,  3488,  7161,\n",
            "          349, 10629,  1269,    12,   175,  1091,   238,  1682,   164,  2148,\n",
            "         1110,  1292,    94,  6331,  6371,     5,   521,   276,   395,  9402,\n",
            "         3597,   294,  1291,  6540,   233,  7671,  7161,   349,    29,   203,\n",
            "            5,   521,  4028,   370,  3988,   249,   233,  3488,  7161,   349,\n",
            "           24,  1529, 11541,   339,     5,   871,   395,  8838,  2063,  8437,\n",
            "            5,   395,  1174,    94,   132,   164,   276,   626,    11,  2491,\n",
            "         4897,  7397,  9468,  3696,  2143,   460,   238,    94,  1389,   164,\n",
            "         1529,   278,   399,   220,   599,  2271,   395,  1174,   278,   164,\n",
            "           94, 11719,   599,  4063,   175,   164,   238, 10963,  6540,    12,\n",
            "          676,   395,    94,   142,     5,  1153,  6410,   136,  2829,   168,\n",
            "          599,  1937,   233,  7671,  7161,   349,  2678,   395,  2865,  1149,\n",
            "          157,   599,  4289,     5,   339,  1981,   121,    94,  1369,   157,\n",
            "         1110,     7,  1119,  1522,   157,   686,  1253,  1529,   395,   225,\n",
            "         9578,   151,   579,   233,  3488,  7161,   349,  1971,   209,  1522,\n",
            "          370,     5,   209,   914,   370,   238, 11328,   168,  2607,   137,\n",
            "          234,  8838, 10183,     5,   294,  4976,     5,   164, 11024,   157,\n",
            "         9356,   443,  2678,  8694,   360,  1150,    15,   402,   772,  2245,\n",
            "          153,     2,   136,  2355,    94,   142,  4949,    11,  2289,  3181,\n",
            "          408,   381,   848,  1226,    15,   164,   136, 12082,  1687,  1797,\n",
            "          349, 11437,     5,  1873,   233,  3488,  7161,   349, 11852,     0,\n",
            "          664,  2800,  1226,  1501,  7671,  7161,   349,    38,  1383,  6011,\n",
            "          125,  5232,  3116,  9689,    46,    12,   616,   276,  8838,  1970,\n",
            "         5695,    94,  1378,   970,   233,  3488,  7161,   349,  1173,   419,\n",
            "          616,  5890,  2379,    11,   794,   493,   136,  2006,   792,   570,\n",
            "         1687,    28,   729,   729,    24,  1569,   349,  2678,   745,   419,\n",
            "            5,   637,  2271,  3654,     5,   157,  1103,    15,  1065,   531,\n",
            "          339,    94,  1767,  5731,   175,  8706,    15,   402,  3285,    15,\n",
            "         4028,     5,   209, 10102,   339,   233,  3488,  7161,   349,  4723,\n",
            "         1888,   225,   370,  5495,   164,   136,  6303,    12,   600,   434,\n",
            "           94, 12011, 10807,  1997,   267,  8518,   335,   871,   408, 11631,\n",
            "          164,   467,   196,  4825,  1107,   408,  2351,  1138,   215,   188,\n",
            "          157,  9438,     7,  1782,  1529,  4667,    94,  4492,   841,   434,\n",
            "         2631,  6220,    64,    11,   600,  3258,  1095,   408,    94,  8743,\n",
            "         2631,  6321,  1697,   233,    28,   729,   729,    24,  1569,   349,\n",
            "         5689,     5,  2548,     5,   637,  1292,  3217,     5,  5790,  5890,\n",
            "         2828,   196,  5571,   339, 11401, 10474,  1501,  3488,  7161,   349,\n",
            "         1285,  2865,     5, 10965,     5,   408,   360,  7087,  1968,   233,\n",
            "           28,   729,   729,    24,  1569,   349,    24,  2360,   339,     5,\n",
            "         3217,     5,  1105,  7665,  1823,    94,  6451,   136,  2348,  1684,\n",
            "         3504,   168,   339,     7,  1357,   508,  4185,   196,  4628,  7677,\n",
            "          157,   267, 10233,   191,     5,   339,   955,   284,  1269,    94,\n",
            "         4609,   354,   136,  9699,   294,   508,   208,  3167,   284,  6389,\n",
            "          856,    94,  8831,   136,  6674,  1223,     5,  4130,  2370,   501,\n",
            "          240,    94,   582,  1253,   278,  3318,     5,  8882,  3704,  7836,\n",
            "         2087,   987,    94,  1631,   596,  2631,  1957,   284,  4733,   844,\n",
            "          460,  2497,    94, 11454,   157,   508,  8196,  2669,     7,  1357,\n",
            "          136, 10233,   191,   196,   582, 11423,     5,   370,   136,  2348,\n",
            "         1684,  3504,     5,   989,   278,     5,   175,    94,  4628, 11850,\n",
            "          164,   856,     5,   370,  6321,     5,  1522,  1149,     7,  1298,\n",
            "          319,   196,  1922,   360,  4523,   386,  7459,   331,    94,   765,\n",
            "         1847,  1065,   596,  5339,   339,     5,   175,   339,   956,    94,\n",
            "          582,  6162,   775,   153,     2,   136,  1223,     5,   664,  1823,\n",
            "          220,   339,   830,  4455,   196,  3064,   339,  4802,   856,   284,\n",
            "         7052,   233,  3488,  7161,   349,   183,   220,   423,     0,  2178,\n",
            "            5,  6702,     0,  1782,   625, 11240,  5514,   220,   423,    94,\n",
            "         7228,    11,  4586,   423,   164,  1604,   604,     5,   175,   651,\n",
            "         2518,  2074,  8970,    94,    48,  1793,  1587,   294, 10133,    12,\n",
            "          989,  1218, 11712,   220,   423,  2399,     5,   164,    94,  9985,\n",
            "          423,  6359,    12,  4621,  4417,   678,  9623,   893,    94,  1313,\n",
            "          147,  1761,   136,  5205,     5,   175,  2438,   596,    94,    61,\n",
            "          981,  3869, 11324,  4417,     5,   164,  5060,   513,   175, 12010,\n",
            "           94,  1378,  4667,     7,  1097,   136, 10416,  4812,   423,   370,\n",
            "          513,     5,   600,   501,    12,   175,    94,  8587,   419,   493,\n",
            "          136,  2139,   600,  6325,   423,   233,    28,   729,   729,    24,\n",
            "         1569,   349,  8148,   339,  1522,    94,  7644,   279, 10474,   145,\n",
            "        11231,  3988,   196,  1671,   238,  6827,   168,  7735,     7,   209,\n",
            "         3258,  2360,   339,    94,    16,  3334,  9959,    11,   278,   955,\n",
            "          238,   339,   434,  4094,   278,   137,  2774,     5,  1838,   278,\n",
            "         8116,   637,  4479,     5,   209,   501, 10486,    94,   969,  8487,\n",
            "          215,    65,   121,  1941,   596,   233,  3488,  7161,   349,  6181])\n"
          ]
        }
      ],
      "source": [
        "from embeddings import MyEncoder\n",
        "\n",
        "my_encoder = MyEncoder(text)\n",
        "data = torch.tensor(my_encoder.tokens, dtype=torch.long)\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SzmNn7ChnzsU"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxmNLNGQoma9",
        "outputId": "bb39fcdf-7b60-4dbd-fd8a-19ca2ab7518a"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "# batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "# block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1 : i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(12111, 10)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from embeddings import NGramLanguageModeler\n",
        "\n",
        "ngmodel = NGramLanguageModeler(my_encoder.vocab_size, embedding_dim=n_embd, context_size=2)\n",
        "ngmodel.load_state_dict(torch.load(\"embedding_model.pt\"))\n",
        "ngmodel.embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpte import BigramLanguageModel\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "model = BigramLanguageModel(my_encoder.vocab_size, embeddings=ngmodel.embeddings)\n",
        "m = model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(logits)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1eF_n_-1q_As"
      },
      "outputs": [],
      "source": [
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyKOyIrgyMmd",
        "outputId": "9b985ca6-abf5-4906-c126-da95078ffc3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 384]) torch.Size([32, 8, 10])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (10) must match the size of tensor b (384) at non-singleton dimension 2",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[39m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m step \u001b[39m==\u001b[39m max_iters \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m         losses \u001b[39m=\u001b[39m estimate_loss()\n\u001b[1;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m: train loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, val loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[39m# sample a batch of data\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[1;32m      8\u001b[0m     X, Y \u001b[39m=\u001b[39m get_batch(split)\n\u001b[0;32m----> 9\u001b[0m     logits, loss \u001b[39m=\u001b[39m model(X, Y)\n\u001b[1;32m     10\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     11\u001b[0m out[split] \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n",
            "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/nanogpt/gpte.py:111\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    109\u001b[0m pos_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_table(torch\u001b[39m.\u001b[39marange(T, device\u001b[39m=\u001b[39mdevice)) \u001b[39m# (T,C)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mprint\u001b[39m(pos_emb\u001b[39m.\u001b[39mshape, tok_emb\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 111\u001b[0m x \u001b[39m=\u001b[39m tok_emb \u001b[39m+\u001b[39;49m pos_emb \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(x) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f(x) \u001b[39m# (B,T,C)\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (384) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "for step in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if step % eval_interval == 0 or step == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    \n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQS_3e85y_E4",
        "outputId": "815b9e1b-940d-4c75-e417-9dbf995c429a"
      },
      "outputs": [],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
